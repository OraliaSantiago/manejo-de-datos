{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <h1 style=\"text-align: center; color: orange;\">RECOMMENDER SYSTEM</h1>\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3 > \n",
    " \n",
    " Los sistemas de recomendación nos ayudan a responder preguntas como ¿Qué película debería rentar? ¿Qué TV debería comprar?\n",
    "\n",
    " Lo primero que debemos responder es:\n",
    " ¿Qué es un sistema de recomendación?\n",
    " \n",
    " Se puede definir como una herramienta que trabaja con información y provee información de items en los que estemos interesados \n",
    "\n",
    " ¿Por qué queremos un sistema de recomendación?\n",
    " \n",
    " Los sistemas de recomendación existen porque a las personas no les gusta perdaer tiempo, lo cual nos ayuda a ahorra tiempo en la busqueda y elección.\n",
    " </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "<h3 style=\"color:orange\"> ¿Cómo funcionan los sistema de recomendación?</h3>\n",
    "\n",
    "\n",
    "<h3> \n",
    "Existen diversas formas de crear un sistema de recomendación, algunos de estos son: \n",
    "\n",
    "* CONTENT-BASED FILTERING (CBF)\n",
    "* COLLABORATIVE- FILTERING (CF)\n",
    "</h3>\n",
    "\n",
    "##\n",
    "<h3 style=\"color:orange\">  CONTENT-BASED FILTERING (CBF) </h3>\n",
    "\n",
    "<h3> \n",
    "Está construido bajo el paradigma: \"muestrame más de lo mismo que me ha gustado\". Por lo cual este enfoque podrá recomentar items que son similares a los que el cliente le ha gustado y las recomendaciones están basadas en las descripciones de los item y las preferencias del perfil del cliente.  El cálculo de la similitud entre los items es lo más importante de esté método y se basa en el contenido de los items.\n",
    "\n",
    "Cuando tratamos con información textual como noticias o libros, se usa el algoritmo *tf-idf* (frequency-inverse document frequency) que representa a una estadística númerica que nos indica la importancia de la palabra para el documento en la colección del corpus (conjunto grande de texto).\n",
    "</h3>\n",
    "\n",
    "\n",
    "##\n",
    "<h3 style=\"color:orange\">  COLLABORATIVE FILTERING (CF) </h3>\n",
    "\n",
    "<h3> \n",
    "CF están construidos con el paradigma: \"Dime que es lo más popular entre los usuarios con gustos similares\". Una hipotesis importante detrás de esté recomendador es que los usuarios similares le gustan los mismos items.\n",
    "Este enfoque se basa en recolectar y analizar un gran número de datos relacionados con el compartamiento de los usuarios, una vez analizados se predice que podría gustarle con base en la similitud de otros usuarios.\n",
    "\n",
    "Una ventaja de este método es que no es necesarion \"entender\"  que item se está recomendanddo. Una desventaja es cuando el sistema no tiene suficiente información del usuario para realizar una inferencia.\n",
    "\n",
    "Existen dos tipos de recomendadores para CF: \n",
    "\n",
    "* USER-BASED: Se encuentran a usuarios similares a mi y se recomienda que es lo que a ellos les gusta. En este método, dado un usuario U, primero encontramos a otros usuarios, que hayan ranquead de forma similar a U y luego se cacula la predicción para U.\n",
    "* ITEM-BASED: Se encuentran los items similares que previamente al usuario le gustó.Y el item basado en CF. Se construye una matriz de items, que ayuda a identificar la relación entre items, después se usa la matriz y la información del usuario U, se realiza la inferencia del gusto del usuario. Este enfoque es útil para casos en los que si un cliente compra X tambien compra Y.\n",
    "</h3>\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "<h3 style=\"color:orange\">  EVALUATION RECOMMENDERS </h3>\n",
    "\n",
    "<h3> \n",
    "La forma más común de evualuar un sistema de recomendación es a través de la predicción, la capacidad de predecir las elecciones de los usuarios. Algunas métricas comunes son RMSE (ROOT MEAN SQUARE ERROR), precisión, recall o ROC.\n",
    "</h3>\n",
    "\n",
    "\n",
    "##\n",
    "<h3 style=\"color:orange\"> PRACTICAL CASE </h3>\n",
    "\n",
    "<h3> \n",
    "El dataset MovieLens es una colección de calificaciones a perliculas a partir de cientos de usuarios , un trabajo de la Universidad de Minnesota, el dataset original consta de 32M de registros y 2M de aplicaciones de etiquetas aplicadas a 87K películas por 200k usuarios. Nosotros trabajaremos con un dataset más reducido de 1M de registros, aproximadamente 3.9K películas y 6K usuarios que se unieron en el año 2000.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "from math import isnan\n",
    "from tqdm import tqdm \n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = pd.read_csv(\"./ml-32m/tags.csv\")\n",
    "# ratings = pd.read_csv(\"./ml-32m/ratings.csv\")\n",
    "# movies = pd.read_csv(\"./ml-32m/movies.csv\")\n",
    "# links = pd.read_csv(\"./ml-32m/links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./ml-32m/\"\n",
    "\n",
    "# user data\n",
    "u_cols = [\"user_id\",\"age\",\"sex\",\"occupation\",\"zip_code\"]\n",
    "users = pd.read_csv(\"./ml-100k/u.user\", sep = \"|\", names = u_cols ,engine='python')\n",
    "\n",
    "# rating data\n",
    "r_cols = [\"user_id\",\"movie_id\",\"rating\",\"unix_timestamp\"]\n",
    "ratings = pd.read_csv(\"./ml-100k/u.data\" , sep = \"\\t\", names = r_cols , engine='python')\n",
    "\n",
    "# movie data\n",
    "m_cols = [\"movie_id\",\"title\",\"release_date\"]\n",
    "movies = pd.read_csv(\"./ml-100k/u.item\", sep = \"|\", names = m_cols ,usecols=range(3) , engine='python', encoding=\"latin-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(pd.merge(users,ratings, on= \"user_id\" , how = \"inner\"),movies, on=\"movie_id\", how = \"inner\")\n",
    "\n",
    "print(\"La tabla users tiene: \", data.user_id.nunique() , \" usuarios únicos.\")\n",
    "print(\"La tabla ratings tiene: \", data.shape[0] , \" ratings.\")\n",
    "print(\"La tabla movies tiene: \", data.movie_id.nunique() , \" películas únicas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['user_id','title', 'movie_id','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[0]/data.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##\n",
    "<h3 style=\"color:orange\"> USER-BASED COLLABORATIVE FILTERING </h3>\n",
    "\n",
    "<h3> \n",
    "En el orden que definimos tenemos \n",
    "\n",
    "1. prediction function. \n",
    "2. user similarity function.\n",
    "3. evaluation function.\n",
    "</h3>\n",
    "\n",
    "##\n",
    "<h3 style=\"color:orange\"> PREDICTION FUNCTION </h3>\n",
    "<h3> \n",
    "La función de predicción atrás del modelo CF se basa en los ratings de las películas de usuarios similares.\n",
    "</h3>\n",
    "\n",
    "* p pertenece al conjunto de películas P.\n",
    "* a un usuario dado.\n",
    "* B conjunto de usuarios.\n",
    "\n",
    "$$pred(a,p) = \\frac{\\sum_{b \\in B}{sim(a,b)*(r_{b,p})}}{\\sum_{b \\in B}{sim(a,b)}}$$\n",
    "\n",
    "<h3> \n",
    "donde sim(a,b) sería la similitud entre los usuarios a y b, B está dado por el conjunto de usuarios que ya vieron la película p, r_{b,p} es el ranking de p dado por el usuario b. \n",
    "</h3>\n",
    "\n",
    "##\n",
    "<h3 style=\"color:orange\"> USER SIMILARITY </h3>\n",
    "\n",
    "<h3> \n",
    "El cálculo de la similitud entre los items es uno de los pasos criticos en el algoritmo CF. La idea básica detrás de la similitud entre los usuarios a y b, es donde primero podemos aislar  el conjunto P de los items que fueron calificados por ambos usuarios y después aplicar la función de similitud.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## user 1 \n",
    "data_user_1 = data[data.user_id==1]\n",
    "\n",
    "## user 6\n",
    "data_user_2 = data[data.user_id==6]\n",
    "\n",
    "## common movies from user 1 and 6\n",
    "common_movies = set(data_user_1.movie_id).intersection(data_user_2.movie_id)\n",
    "\n",
    "print(\"Número de películas en común:\", len(common_movies))\n",
    "\n",
    "mask = data_user_1.movie_id.isin(common_movies)\n",
    "data_user_1 = data_user_1[mask]\n",
    "print(data_user_1[[\"title\",\"rating\"]])\n",
    "\n",
    "mask = data_user_2.movie_id.isin(common_movies)\n",
    "data_user_2 = data_user_2[mask]\n",
    "print(data_user_2[[\"title\",\"rating\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> \n",
    "Una vez obtenidos las calificaciones de las películas en común de dos usuarios, podemos calculas la similitud, estas son formas comunes de calcularla: \n",
    "</h3>\n",
    "\n",
    "<ul>\n",
    "<li>Distancia Euclidiana</li>\n",
    "\n",
    "$$sim(a,b) = \\frac{1}{1+\\sqrt{\\sum_{p \\in P}{(r_{a,p} - r_{b,p})^2}}}$$\n",
    "<br>\n",
    "<li>Correlación de Pearson</li>\n",
    "\n",
    "$$sim(a,b) = \\frac{\\sum_{p\\in P} (r_{a,p}-\\bar{r_a})(r_{b,p}-\\bar{r_b})}{\\sqrt{\\sum_{p \\in P}(r_{a,p}-\\bar{r_a})²}\\sqrt{\\sum_{p \\in P}(r_{b,p}-\\bar{r_b})²}}$$\n",
    "<br>\n",
    "<li>Distancia coseno (Similitud coseno) </li>\n",
    "\n",
    "$$ sim(a,b) = \\frac{\\vec{a}· \\vec{b}}{|\\vec{a}| * |\\vec{b}|}$$\n",
    "<br>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "Donde: \n",
    "\n",
    "* $sim(a,b)$ es la similitud entre el usuario \"a\" y el \"b\".\n",
    "* $P$ es el conjunto de peliculas calificadas en común por los usuarios \"a\" y \"b\"\n",
    "* $r_{a,p}$ es la calificación de la película \"p\" por el usuario \"a\"\n",
    "* $\\bar{r_a}$ es la media de las calificaciones por el usuario \"a\"\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<h4>Algunos problemas</h4>\n",
    "<li>La correlación de pearson es mejor que la distancia euclidiana ya que se basa más en las calificaciones que en los valores.</li>\n",
    "<li>La distanicia coseno es usualmente usada cuando los datos son binarios o unitarios.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_sim(df, user_1, user_2, \n",
    "                    min_common_items = 3, \n",
    "                    method = 'pearson'):\n",
    "    # GET MOVIES OF USER1\n",
    "    mov_usr1 = df[df['user_id'] == user_1 ]\n",
    "    # GET MOVIES OF USER2\n",
    "    mov_usr2 = df[df['user_id'] == user_2 ]\n",
    "    \n",
    "    # FIND SHARED FILMS\n",
    "    df_shared = pd.merge(mov_usr1, mov_usr2, on='movie_id')  \n",
    "    \n",
    "    # If there is no enough common items to comput similarity\n",
    "    if(df_shared.shape[0]<=min_common_items):\n",
    "        return 0\n",
    "    \n",
    "    if method =='pearson':\n",
    "        res = pearsonr(df_shared['rating_x'],df_shared['rating_y'])[0]\n",
    "        if(np.isnan(res)):\n",
    "            return 0\n",
    "        return res\n",
    "    \n",
    "    elif method =='euclidean':\n",
    "        return 1.0/(1.0+euclidean(df_shared['rating_x'],\n",
    "                                  df_shared['rating_y'])) \n",
    "    else:\n",
    "        print(\"method not defined\")\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean Similarity\",\n",
    "      user_sim(data,1,8,method = 'euclidean') )\n",
    "print(\"Pearson Similarity\",user_sim(data,1,8) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean Similarity\",\n",
    "      user_sim(data,1,31, method = 'euclidean') )\n",
    "print(\"Pearson Similarity\",user_sim(data,1,31) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_1, user_id_2 = 3, 8\n",
    "\n",
    "movies_user1=data[data['user_id'] ==user_id_1 ][['user_id','movie_id','rating']]\n",
    "movies_user2=data[data['user_id'] ==user_id_2 ][['user_id','movie_id','rating']]\n",
    "    \n",
    "# FIND SHARED FILMS\n",
    "rep=pd.merge(movies_user1 ,movies_user2,on='movie_id')\n",
    "x= rep.rating_x + np.random.normal(loc=0.0, scale=0.1,size=len(rep.rating_x))\n",
    "y= rep.rating_y +np.random.normal(loc=0.0, scale=0.1,size=len(rep.rating_y))\n",
    "    \n",
    "a=rep.groupby(['rating_x', 'rating_y']).size()\n",
    "x=[]\n",
    "y=[]\n",
    "s=[]\n",
    "for item,b in a.items():\n",
    "    x.append(item[0])\n",
    "    y.append(item[1])\n",
    "    s.append(b*30.)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.scatter(x,y, s=s)\n",
    "plt.xlabel('Rating User 3')\n",
    "plt.ylabel('Rating User '+str(8))\n",
    "plt.axis([0.5,5.5,0.5,5.5])\n",
    "# plt.savefig(\"corre18.png\",dpi= 300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_1, user_id_2 = 3, 31\n",
    "movies_user1=data[data['user_id'] ==user_id_1 ][['user_id','movie_id','rating']]\n",
    "movies_user2=data[data['user_id'] ==user_id_2 ][['user_id','movie_id','rating']]\n",
    "    \n",
    "# FIND SHARED FILMS\n",
    "rep=pd.merge(movies_user1 ,movies_user2,on='movie_id')\n",
    "x= rep.rating_x + np.random.normal(loc=0.0, scale=0.1,size=len(rep.rating_x))\n",
    "y= rep.rating_y +np.random.normal(loc=0.0, scale=0.1,size=len(rep.rating_y))\n",
    "    \n",
    "a=rep.groupby(['rating_x', 'rating_y']).size()\n",
    "x=[]\n",
    "y=[]\n",
    "s=[]\n",
    "for item,b in a.items():\n",
    "    x.append(item[0])\n",
    "    y.append(item[1])\n",
    "    s.append(b*30.)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.scatter(x,y, s=s)\n",
    "plt.xlabel('Rating User 3')\n",
    "plt.ylabel('Rating User '+str(31))\n",
    "plt.axis([0.5,5.5,0.5,5.5])\n",
    "# plt.savefig(\"corre131.png\",dpi= 300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>EVALUACIÓN DEL MODELO</h3>\n",
    "\n",
    "Dividiremos el datasets principal en dos diferente, uno llamado $X_{train}$ con el 80\\% de los datos y otro llamado $X_{test}$ con el 20\\% restante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_set(df):\n",
    "    sampled_ids = np.random.choice(\n",
    "        df.index,\n",
    "        size=np.int64(np.ceil(df.index.size * 0.2)),\n",
    "        replace=False)\n",
    "    df.loc[sampled_ids, 'for_testing'] = True\n",
    "    return df\n",
    "\n",
    "def create_train_test(data, key = 'user_id'):\n",
    "    data['for_testing'] = False\n",
    "    grouped = data.groupby(key, group_keys=False).apply(assign_to_set,include_groups=False)\n",
    "    # dataframe used to train our model\n",
    "    data_train = data[grouped.for_testing == False]\n",
    "    # dataframe used to evaluate our model\n",
    "    data_test = data[grouped.for_testing == True]\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "X_train, X_test =  create_train_test(data)\n",
    "\n",
    "print(\"#Training samples = \",X_train.shape[0])\n",
    "print(\"#Test samples = \",X_test.shape[0])\n",
    "print('#Users =', X_train.user_id.nunique())\n",
    "print('#Movies =',X_train.movie_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "<h3 style=\"color:orange\"> MÉTRICAS DE EVALUACIÓN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_pred, y_true):\n",
    "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, \n",
    "                          relevant_items, \n",
    "                          assume_unique=True)\n",
    "    \n",
    "    precision = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def recall(recommended_items, relevant_items):  \n",
    "    is_relevant = np.in1d(recommended_items, \n",
    "                          relevant_items, \n",
    "                          assume_unique=True)\n",
    "    \n",
    "    recall = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def AP(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, \n",
    "                          relevant_items, \n",
    "                          assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    ap_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return ap_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rec_object, train, test, \n",
    "             at = 20, thr_relevant= 4):\n",
    "    \"\"\" Perfomance evaluation \"\"\"\n",
    "    \n",
    "    # RMSE evaluation\n",
    "    ids_to_estimate = zip(test.user_id, test.movie_id)\n",
    "    y_estimated = np.array([rec_object.predict_score(u,i) \n",
    "                          if u in train.user_id else 3 \n",
    "                          for (u,i) in ids_to_estimate ])\n",
    "    \n",
    "    y_real = test.rating.values\n",
    "    rmse = compute_rmse(y_estimated, y_real)\n",
    "    \n",
    "    print(\"RMSE: {:.4f}\".format(rmse))\n",
    "          \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_AP = 0.0\n",
    "\n",
    "    num_eval = 0\n",
    "\n",
    "    for user_id in tqdm(test.user_id.unique()):\n",
    "\n",
    "        relevant_items = test[(test.user_id==user_id )&( test.rating>=thr_relevant)].movie_id.values\n",
    "\n",
    "        if len(relevant_items)>0:\n",
    "\n",
    "            recommended_items = rec_object.predict_top(user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_AP += AP(recommended_items, relevant_items)\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    MAP = cumulative_AP / num_eval\n",
    "\n",
    "    print(\"Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, MAP)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "<h3 style=\"color:orange\"> NAIVE RECOMMENDER SYSTEM</h3>\n",
    "\n",
    "<h3>\n",
    "Primero veamos los resultados de un sistema de recomendación aleatorio (random).\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRecommender():\n",
    "\n",
    "    def fit(self, train):\n",
    "        self.items = train.title.unique()\n",
    "    \n",
    "    def predict_score(self, user_id, movie_id):\n",
    "        '''Given a user_id and movie_id predict its score'''\n",
    "        return np.random.uniform(1,5)\n",
    "    \n",
    "    def predict_top(self, user_id, at=5):\n",
    "        '''Given a user_id predicts its top 'at' movies'''\n",
    "        recommended_items = np.random.choice(self.items, at)\n",
    "\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model = RandomRecommender()\n",
    "random_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(random_model,X_train,X_test, at = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "El modelo aleatorio nos da un RMSE de 1.63 y las demás métricas del 0%\n",
    "</h3>\n",
    "\n",
    "<h3>\n",
    "Ahora construimos el sistema de recomendación con CF, \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering:\n",
    "    \"\"\" Collaborative filtering model \"\"\"\n",
    "    \n",
    "    def __init__(self, similarity = 'pearson'):\n",
    "        '''Constructor'''\n",
    "        self.sim_method=similarity\n",
    "        self.sim = {}\n",
    "\n",
    "    def fit(self, train):\n",
    "        '''Prepare data structures for estimation'''\n",
    "        self.train = train\n",
    "        allUsers = set(self.train['user_id'])\n",
    "        \n",
    "        # Create a dictionary with user, movie, rating\n",
    "        self.seen_movies = {user: {} for user in allUsers}\n",
    "            \n",
    "        for user in allUsers:\n",
    "            user_ratings = train[train.user_id == user][['movie_id', 'rating']]\n",
    "            self.seen_movies[user] = dict(zip(user_ratings['movie_id'], user_ratings['rating']))\n",
    "    \n",
    "        for usr_1 in tqdm(allUsers):\n",
    "            self.sim.setdefault(usr_1, {})\n",
    "            a = self.train[self.train.user_id == usr_1][['movie_id']]\n",
    "            data_reduced = pd.merge(self.train, a , on='movie_id') \n",
    "            \n",
    "            for usr_2 in allUsers:\n",
    "                # Avoid comparing a user with themselves\n",
    "                if usr_1 == usr_2: \n",
    "                    continue      \n",
    "                self.sim.setdefault(usr_2, {})\n",
    "                if(usr_1 in self.sim[usr_2]):\n",
    "                    continue # since is a simetric matrix\n",
    "                sim = user_sim(data_reduced, usr_1, usr_2, \n",
    "                               method = self.sim_method)\n",
    "                if(sim < 0):\n",
    "                    self.sim[usr_1][usr_2]=0\n",
    "                    self.sim[usr_2][usr_1]=0\n",
    "                else:\n",
    "                    self.sim[usr_1][usr_2]=sim\n",
    "                    self.sim[usr_2][usr_1]=sim\n",
    "        \n",
    "        \n",
    "    def predict_score(self, usr_id, movie_id):\n",
    "        ''' Given a user_id and movie_id it predicts its score'''\n",
    "        seen = self.train[self.train.movie_id == movie_id]\n",
    "        rating_num, rating_den = 0.0, 0.0\n",
    "        allUsers = set(seen['user_id'])\n",
    "        # Iterate through all users who have seen the movie\n",
    "        for other in allUsers:\n",
    "            if usr_id == other:\n",
    "                continue  # Skip the current user, as self-similarity is not needed\n",
    "            \n",
    "            similarity = self.sim[usr_id][other]\n",
    "            rating = float(self.seen_movies[other][movie_id])\n",
    "            \n",
    "            rating_num += similarity * rating\n",
    "            rating_den += similarity\n",
    "            \n",
    "        # If the denominator is zero (no similar users), handle the case\n",
    "        if rating_den == 0: \n",
    "            if self.train.rating[self.train['movie_id'] == movie_id].mean()>0:\n",
    "                # return the mean movie rating if there is no similar for the computation\n",
    "                return self.train.rating[self.train['movie_id'] == movie_id].mean()\n",
    "            else:# return mean user rating \n",
    "                return self.train.rating[self.train['user_id'] == usr_id].mean()\n",
    "       \n",
    "        # return the predicted score\n",
    "        return rating_num/rating_den\n",
    "\n",
    "    def predict_top(self, usr_id, at=20, remove_seen=True):\n",
    "        '''Given a usr_id predict its top 'at' movies'''\n",
    "        # Get the movies already seen by the user\n",
    "        seen_items = set(self.train[self.train.user_id == usr_id].movie_id.values)\n",
    "\n",
    "        # Get the set of unseen items\n",
    "        unseen_items = set(self.train.movie_id.values) - seen_items\n",
    "\n",
    "        # Generate predictions for unseen items\n",
    "        predictions = [(item_id, self.predict_score(usr_id, item_id)) for item_id in unseen_items]\n",
    "\n",
    "        # Sort predictions by score in descending order and select the top 'at' items\n",
    "        sorted_predictions = sorted(predictions, key=lambda x: x[1], reverse=True)[:at]\n",
    "\n",
    "        # Extract the item IDs from the sorted predictions\n",
    "        top_items = [item_id for item_id, _ in sorted_predictions]\n",
    "        \n",
    "        return top_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model = CollaborativeFiltering()\n",
    "cf_model.fit(X_train)\n",
    "cf_model.predict_score(usr_id = 2, movie_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se tarda en procesar 8 horas\n",
    "# evaluate(cf_model,X_train,X_test, at = 50)\n",
    "\n",
    "# RMSE: 1.0630\n",
    "\n",
    "# Precision = 0.0197, Recall = 0.0687, MAP = 0.0032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> \n",
    "Sabemos que las calificaciones de las personas no tienen el mismos compartamiento que los críticos, algunos califican con mayor o menor puntuación, la siguiente función toma en cuenta la media del usuarios.\n",
    "\n",
    "$$pred(a,p) = \\bar{r_a} + \\frac{\\sum_{b \\in N}{sim(a,b)*(r_{b,p}-\\bar{r_b})}}{\\sum_{b \\in N}{sim(a,b)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering_Ex1(CollaborativeFiltering):\n",
    "    def predict_score(self, usr_id, movie_id, N = 10):\n",
    "        ''' Given a user_id and movie_id it predicts its score'''\n",
    "        seen = self.train[self.train['movie_id'] ==movie_id]\n",
    "        rating_num, rating_den = 0.0, 0.0\n",
    "        allUsers = set(seen['user_id'])\n",
    "        \n",
    "        for other in allUsers:\n",
    "            if usr_id == other:\n",
    "                continue  # Skip the current user, as self-similarity is not needed\n",
    "            similarity = self.sim[usr_id][other]\n",
    "            rating = float(self.seen_movies[other][movie_id])\n",
    "            mean_user_rating = np.mean([self.seen_movies[other][key] \n",
    "                                        for key in self.seen_movies[other]])\n",
    "            rating_num += similarity * float(rating - mean_user_rating)  \n",
    "            rating_den += similarity\n",
    "        if rating_den == 0: \n",
    "            if self.train.rating[self.train['movie_id'] == movie_id].mean()>0:\n",
    "                # return the mean movie rating if there is no similar for the computation\n",
    "                return self.train.rating[self.train['movie_id'] == movie_id].mean()\n",
    "            else:\n",
    "                # else return mean user rating \n",
    "                return self.train.rating[self.train['user_id'] == usr_id].mean()\n",
    "        mean_rating_user = self.train[self.train.user_id==usr_id].rating.mean()\n",
    "        return mean_rating_user + rating_num/rating_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model_v2 = CollaborativeFiltering_Ex1()\n",
    "cf_model_v2.fit(X_train)\n",
    "cf_model_v2.predict_score(usr_id = 2, movie_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(cf_model_v2, X_train, X_test, at = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
